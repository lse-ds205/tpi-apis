{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPI Database creation \n",
    "\n",
    "This notebook creates the SQL database for the TPI data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd \n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Add the project root to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Now import with full module path\n",
    "from utils.database_creation_utils import get_db_connection, get_engine\n",
    "\n",
    "engine = get_engine()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sector Entities \n",
    "\n",
    "We began by creating the sector table as it was the least connected to the other entities and therefore it is the logical starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created and populated successfully.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/TPI_sector_data_All_sectors_08032025/Sector_Benchmarks_08032025.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Drop and create tables\n",
    "create_tables_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS benchmark_projection;\n",
    "DROP TABLE IF EXISTS sector_benchmark;\n",
    "\n",
    "CREATE TABLE sector_benchmark (\n",
    "  benchmark_id VARCHAR NOT NULL,\n",
    "  sector_name VARCHAR NOT NULL,\n",
    "  scenario_name VARCHAR NOT NULL,\n",
    "  region VARCHAR NOT NULL,\n",
    "  release_date DATE NOT NULL,\n",
    "  unit VARCHAR NOT NULL,\n",
    "  PRIMARY KEY (benchmark_id, sector_name, scenario_name)\n",
    ");\n",
    "\n",
    "CREATE TABLE benchmark_projection (\n",
    "  benchmark_projection_year INT NOT NULL,\n",
    "  benchmark_projection_attribute FLOAT,\n",
    "  benchmark_id VARCHAR NOT NULL,\n",
    "  sector_name VARCHAR NOT NULL,\n",
    "  scenario_name VARCHAR NOT NULL,\n",
    "  FOREIGN KEY (benchmark_id, sector_name, scenario_name) \n",
    "    REFERENCES sector_benchmark(benchmark_id, sector_name, scenario_name)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_tables_sql))\n",
    "    conn.commit()\n",
    "\n",
    "# Prepare Sector_Benchmark data\n",
    "sector_benchmark_df = df[['Benchmark ID', 'Sector name', 'Scenario name', 'Region', 'Release date', 'Unit']].drop_duplicates()\n",
    "sector_benchmark_df.columns = ['benchmark_id', 'sector_name', 'scenario_name', 'region', 'release_date', 'unit']\n",
    "sector_benchmark_df['release_date'] = pd.to_datetime(sector_benchmark_df['release_date'], dayfirst=True)\n",
    "\n",
    "# Create composite key for FK matching\n",
    "sector_benchmark_df['benchmark_key'] = (\n",
    "    sector_benchmark_df['benchmark_id'] + '||' +\n",
    "    sector_benchmark_df['sector_name'] + '||' +\n",
    "    sector_benchmark_df['scenario_name']\n",
    ")\n",
    "\n",
    "# Insert into sector_benchmark\n",
    "sector_benchmark_df.drop(columns=['benchmark_key']).to_sql('sector_benchmark', engine, if_exists='append', index=False)\n",
    "\n",
    "# Prepare benchmark_projection\n",
    "projection_years = [str(y) for y in range(2013, 2051)]\n",
    "benchmark_projection_df = df.melt(\n",
    "    id_vars=['Benchmark ID', 'Sector name', 'Scenario name'],\n",
    "    value_vars=projection_years,\n",
    "    var_name='benchmark_projection_year',\n",
    "    value_name='benchmark_projection_attribute'\n",
    ")\n",
    "benchmark_projection_df.columns = ['benchmark_id', 'sector_name', 'scenario_name', 'benchmark_projection_year', 'benchmark_projection_attribute']\n",
    "benchmark_projection_df['benchmark_projection_year'] = benchmark_projection_df['benchmark_projection_year'].astype(int)\n",
    "\n",
    "# Create composite key for join check\n",
    "benchmark_projection_df['benchmark_key'] = (\n",
    "    benchmark_projection_df['benchmark_id'] + '||' +\n",
    "    benchmark_projection_df['sector_name'] + '||' +\n",
    "    benchmark_projection_df['scenario_name']\n",
    ")\n",
    "\n",
    "# Enforce FK integrity\n",
    "valid_keys = sector_benchmark_df['benchmark_key'].unique()\n",
    "benchmark_projection_df = benchmark_projection_df[benchmark_projection_df['benchmark_key'].isin(valid_keys)]\n",
    "\n",
    "# Drop rows with missing projection values\n",
    "benchmark_projection_df = benchmark_projection_df.dropna(subset=['benchmark_projection_attribute'])\n",
    "\n",
    "# Final insert\n",
    "benchmark_projection_df.drop(columns=['benchmark_key'], inplace=True)\n",
    "benchmark_projection_df.to_sql('benchmark_projection', engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Tables created and populated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "(psycopg2.errors.DependentObjectsStillExist) cannot drop table company because other objects depend on it\nDETAIL:  constraint cp_assessment_company_name_version_fkey on table cp_assessment depends on table company\nconstraint mq_assessment_company_name_version_fkey on table mq_assessment depends on table company\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n\n[SQL: \nDROP TABLE IF EXISTS company_answer;\nDROP TABLE IF EXISTS company;\n\nCREATE TABLE company (\n  company_name VARCHAR NOT NULL,\n  geography VARCHAR,\n  isin VARCHAR,\n  ca100_focus VARCHAR,\n  size_classification VARCHAR,\n  geography_code VARCHAR,\n  sedol VARCHAR,\n  version VARCHAR NOT NULL,\n  sector_name VARCHAR,\n  PRIMARY KEY (company_name, version)\n);\n\nCREATE TABLE company_answer (\n  question_code VARCHAR NOT NULL,\n  question_text VARCHAR NOT NULL,\n  response VARCHAR NOT NULL,\n  company_name VARCHAR NOT NULL,\n  version VARCHAR NOT NULL,\n  PRIMARY KEY (question_code, company_name, version),\n  FOREIGN KEY (company_name, version) REFERENCES company(company_name, version)\n);\n]\n(Background on this error at: https://sqlalche.me/e/20/2j85)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDependentObjectsStillExist\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1967\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine._has_events:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:924\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mDependentObjectsStillExist\u001b[39m: cannot drop table company because other objects depend on it\nDETAIL:  constraint cp_assessment_company_name_version_fkey on table cp_assessment depends on table company\nconstraint mq_assessment_company_name_version_fkey on table mq_assessment depends on table company\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     10\u001b[39m create_company_tables_sql = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33mDROP TABLE IF EXISTS company_answer;\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33mDROP TABLE IF EXISTS company;\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m \u001b[33m);\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m engine.connect() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_company_tables_sql\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     conn.commit()\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Map metadata columns\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1418\u001b[39m, in \u001b[36mConnection.execute\u001b[39m\u001b[34m(self, statement, parameters, execution_options)\u001b[39m\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1419\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:515\u001b[39m, in \u001b[36mClauseElement._execute_on_connection\u001b[39m\u001b[34m(self, connection, distilled_params, execution_options)\u001b[39m\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1640\u001b[39m, in \u001b[36mConnection._execute_clauseelement\u001b[39m\u001b[34m(self, elem, distilled_parameters, execution_options)\u001b[39m\n\u001b[32m   1628\u001b[39m compiled_cache: Optional[CompiledCacheType] = execution_options.get(\n\u001b[32m   1629\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompiled_cache\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.engine._compiled_cache\n\u001b[32m   1630\u001b[39m )\n\u001b[32m   1632\u001b[39m compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(\n\u001b[32m   1633\u001b[39m     dialect=dialect,\n\u001b[32m   1634\u001b[39m     compiled_cache=compiled_cache,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1638\u001b[39m     linting=\u001b[38;5;28mself\u001b[39m.dialect.compiler_linting | compiler.WARN_LINTING,\n\u001b[32m   1639\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1640\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_execute(\n\u001b[32m   1654\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1655\u001b[39m         elem,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1659\u001b[39m         ret,\n\u001b[32m   1660\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1846\u001b[39m, in \u001b[36mConnection._execute_context\u001b[39m\u001b[34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[39m\n\u001b[32m   1844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exec_insertmany_context(dialect, context)\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1846\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1986\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1983\u001b[39m     result = context._setup_result_proxy()\n\u001b[32m   1985\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2353\u001b[39m, in \u001b[36mConnection._handle_dbapi_exception\u001b[39m\u001b[34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[39m\n\u001b[32m   2351\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[32m   2352\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2353\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001b[32m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2354\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2355\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[32m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1967\u001b[39m, in \u001b[36mConnection._exec_single_context\u001b[39m\u001b[34m(self, dialect, context, statement, parameters)\u001b[39m\n\u001b[32m   1965\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine._has_events:\n\u001b[32m   1972\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch.after_cursor_execute(\n\u001b[32m   1973\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1974\u001b[39m         cursor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1978\u001b[39m         context.executemany,\n\u001b[32m   1979\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DS205/tpi-apis/venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:924\u001b[39m, in \u001b[36mDefaultDialect.do_execute\u001b[39m\u001b[34m(self, cursor, statement, parameters, context)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m     \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError\u001b[39m: (psycopg2.errors.DependentObjectsStillExist) cannot drop table company because other objects depend on it\nDETAIL:  constraint cp_assessment_company_name_version_fkey on table cp_assessment depends on table company\nconstraint mq_assessment_company_name_version_fkey on table mq_assessment depends on table company\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n\n[SQL: \nDROP TABLE IF EXISTS company_answer;\nDROP TABLE IF EXISTS company;\n\nCREATE TABLE company (\n  company_name VARCHAR NOT NULL,\n  geography VARCHAR,\n  isin VARCHAR,\n  ca100_focus VARCHAR,\n  size_classification VARCHAR,\n  geography_code VARCHAR,\n  sedol VARCHAR,\n  version VARCHAR NOT NULL,\n  sector_name VARCHAR,\n  PRIMARY KEY (company_name, version)\n);\n\nCREATE TABLE company_answer (\n  question_code VARCHAR NOT NULL,\n  question_text VARCHAR NOT NULL,\n  response VARCHAR NOT NULL,\n  company_name VARCHAR NOT NULL,\n  version VARCHAR NOT NULL,\n  PRIMARY KEY (question_code, company_name, version),\n  FOREIGN KEY (company_name, version) REFERENCES company(company_name, version)\n);\n]\n(Background on this error at: https://sqlalche.me/e/20/2j85)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define paths to company files\n",
    "file_5 = \"../data/TPI_sector_data_All_sectors_08032025/Company_Latest_Assessments_5.0.csv\"\n",
    "file_4 = \"../data/TPI_sector_data_All_sectors_08032025/Company_Latest_Assessments.csv\"\n",
    "\n",
    "# Load the files\n",
    "df_5 = pd.read_csv(file_5)\n",
    "df_4 = pd.read_csv(file_4)\n",
    "\n",
    "# Define and run SQL to create the tables\n",
    "create_company_tables_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS company_answer;\n",
    "DROP TABLE IF EXISTS company;\n",
    "\n",
    "CREATE TABLE company (\n",
    "  company_name VARCHAR NOT NULL,\n",
    "  geography VARCHAR,\n",
    "  isin VARCHAR,\n",
    "  ca100_focus VARCHAR,\n",
    "  size_classification VARCHAR,\n",
    "  geography_code VARCHAR,\n",
    "  sedol VARCHAR,\n",
    "  version VARCHAR NOT NULL,\n",
    "  sector_name VARCHAR,\n",
    "  PRIMARY KEY (company_name, version)\n",
    ");\n",
    "\n",
    "CREATE TABLE company_answer (\n",
    "  question_code VARCHAR NOT NULL,\n",
    "  question_text VARCHAR NOT NULL,\n",
    "  response VARCHAR NOT NULL,\n",
    "  company_name VARCHAR NOT NULL,\n",
    "  version VARCHAR NOT NULL,\n",
    "  PRIMARY KEY (question_code, company_name, version),\n",
    "  FOREIGN KEY (company_name, version) REFERENCES company(company_name, version)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_company_tables_sql))\n",
    "    conn.commit()\n",
    "\n",
    "# Map metadata columns\n",
    "meta_cols_common = {\n",
    "    'Company Name': 'company_name',\n",
    "    'Geography': 'geography',\n",
    "    'Geography Code': 'geography_code',\n",
    "    'Sector': 'sector_name',\n",
    "    'CA100 Focus Company': 'ca100_focus',\n",
    "    'Large/Medium Classification': 'size_classification',\n",
    "    'ISINs': 'isin',\n",
    "    'SEDOL': 'sedol'\n",
    "}\n",
    "\n",
    "# Extract and tag company data\n",
    "df_5_meta = df_5[list(meta_cols_common.keys())].rename(columns=meta_cols_common)\n",
    "df_4_meta = df_4[list(meta_cols_common.keys())].rename(columns=meta_cols_common)\n",
    "df_5_meta[\"version\"] = \"5.0\"\n",
    "df_4_meta[\"version\"] = \"4.0\"\n",
    "company_df = pd.concat([df_4_meta, df_5_meta], ignore_index=True)\n",
    "\n",
    "# Insert into company table\n",
    "company_df.to_sql(\"company\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "# Build company_answer DataFrame\n",
    "def extract_answers(df, version):\n",
    "    questions = [col for col in df.columns if col.startswith(\"Q\") and \"|\" in col]\n",
    "    records = []\n",
    "    for q in questions:\n",
    "        code, text = q.split(\"|\", 1)\n",
    "        for _, row in df.iterrows():\n",
    "            records.append({\n",
    "                \"question_code\": code.strip(),\n",
    "                \"question_text\": text.strip(),\n",
    "                \"response\": row[q],\n",
    "                \"company_name\": row[\"Company Name\"].strip(),\n",
    "                \"version\": version\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "answers_4 = extract_answers(df_4, \"4.0\")\n",
    "answers_5 = extract_answers(df_5, \"5.0\")\n",
    "company_answer_df = pd.concat([answers_4, answers_5], ignore_index=True)\n",
    "\n",
    "# Drop null responses (required for NOT NULL constraint)\n",
    "company_answer_df = company_answer_df.dropna(subset=[\"response\"])\n",
    "\n",
    "# Enforce FK constraint: only include answers for existing companies\n",
    "valid_keys = set(zip(company_df['company_name'], company_df['version']))\n",
    "company_answer_df = company_answer_df[\n",
    "    company_answer_df.apply(lambda row: (row['company_name'], row['version']) in valid_keys, axis=1)\n",
    "]\n",
    "\n",
    "# Insert into company_answer table\n",
    "company_answer_df.to_sql(\"company_answer\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "print(\"✅ Company and answer tables created and populated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MQ Assessments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MQ assessments table created and populated.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mq_dir = \"../data/TPI_sector_data_All_sectors_08032025\"\n",
    "\n",
    "# Dynamically find MQ files and extract tpi_cycle\n",
    "mq_files = {}\n",
    "for fname in os.listdir(mq_dir):\n",
    "    match = re.match(r\"MQ_Assessments_Methodology_(\\d+)_.*\\.csv\", fname)\n",
    "    if match:\n",
    "        cycle = int(match.group(1))\n",
    "        mq_files[cycle] = os.path.join(mq_dir, fname)\n",
    "\n",
    "# Load latest company versions\n",
    "with engine.connect() as conn:\n",
    "    company_df = pd.read_sql(\"SELECT company_name, MAX(version) as version FROM company GROUP BY company_name\", conn)\n",
    "\n",
    "# Create MQ table\n",
    "create_mq_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS mq_assessment;\n",
    "\n",
    "CREATE TABLE mq_assessment (\n",
    "  assessment_date DATE,\n",
    "  publication_date DATE,\n",
    "  level INT,\n",
    "  performance_change VARCHAR,\n",
    "  tpi_cycle INT NOT NULL,\n",
    "  company_name VARCHAR NOT NULL,\n",
    "  version VARCHAR NOT NULL,\n",
    "  PRIMARY KEY (company_name, version, tpi_cycle, assessment_date),\n",
    "  FOREIGN KEY (company_name, version) REFERENCES company(company_name, version)\n",
    ");\n",
    "\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_mq_sql))\n",
    "    conn.commit()\n",
    "\n",
    "# Process all MQ files\n",
    "mq_records = []\n",
    "for cycle, path in mq_files.items():\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df['tpi_cycle'] = cycle\n",
    "    df['company_name'] = df['Company Name'].str.strip()\n",
    "    df['assessment_date'] = pd.to_datetime(df['Assessment Date'], dayfirst=True, errors='coerce')\n",
    "    df['publication_date'] = pd.to_datetime(df['Publication Date'], errors='coerce')\n",
    "    df['level'] = pd.to_numeric(df['Level'], errors='coerce')\n",
    "    df['performance_change'] = df['Performance compared to previous year'].astype(str)\n",
    "    df = df[['company_name', 'assessment_date', 'publication_date', 'level', 'performance_change', 'tpi_cycle']]\n",
    "    mq_records.append(df)\n",
    "\n",
    "mq_df = pd.concat(mq_records, ignore_index=True)\n",
    "\n",
    "# Merge with most recent company version\n",
    "mq_df = mq_df.merge(company_df, on='company_name', how='inner')\n",
    "\n",
    "# Final insert\n",
    "mq_df.to_sql(\"mq_assessment\", engine, if_exists=\"append\", index=False)\n",
    "\n",
    "print(\"✅ MQ assessments table created and populated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP Assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CP tables created and populated successfully.\n"
     ]
    }
   ],
   "source": [
    "cp_dir = \"../data/TPI_sector_data_All_sectors_08032025\"\n",
    "\n",
    "\n",
    "# Step 1: Create CP tables\n",
    "create_cp_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS cp_projection;\n",
    "DROP TABLE IF EXISTS cp_alignment;\n",
    "DROP TABLE IF EXISTS cp_assessment;\n",
    "\n",
    "CREATE TABLE cp_assessment (\n",
    "  assessment_date DATE NOT NULL,\n",
    "  publication_date DATE,\n",
    "  assumptions VARCHAR,\n",
    "  cp_unit VARCHAR,\n",
    "  projection_cutoff DATE,\n",
    "  benchmark_id VARCHAR,\n",
    "  is_regional VARCHAR NOT NULL,\n",
    "  company_name VARCHAR NOT NULL,\n",
    "  version VARCHAR NOT NULL,\n",
    "  PRIMARY KEY (assessment_date, is_regional, company_name, version),\n",
    "  FOREIGN KEY (company_name, version) REFERENCES company(company_name, version)\n",
    ");\n",
    "\n",
    "CREATE TABLE cp_alignment (\n",
    "  cp_alignment_year INT NOT NULL,\n",
    "  cp_alignment_value VARCHAR NOT NULL,\n",
    "  assessment_date DATE NOT NULL,\n",
    "  company_name VARCHAR NOT NULL,\n",
    "  version VARCHAR NOT NULL,\n",
    "  is_regional VARCHAR NOT NULL,\n",
    "  FOREIGN KEY (assessment_date, company_name, version, is_regional)\n",
    "    REFERENCES cp_assessment(assessment_date, company_name, version, is_regional)\n",
    ");\n",
    "\n",
    "CREATE TABLE cp_projection (\n",
    "  cp_projection_year INT NOT NULL,\n",
    "  cp_projection_value INT NOT NULL,\n",
    "  assessment_date DATE NOT NULL,\n",
    "  company_name VARCHAR NOT NULL,\n",
    "  version VARCHAR NOT NULL,\n",
    "  is_regional VARCHAR NOT NULL,\n",
    "  FOREIGN KEY (assessment_date, company_name, version, is_regional)\n",
    "    REFERENCES cp_assessment(assessment_date, company_name, version, is_regional)\n",
    ");\n",
    "\"\"\"\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(create_cp_sql))\n",
    "    conn.commit()\n",
    "\n",
    "# Step 2: Load company versions\n",
    "with engine.connect() as conn:\n",
    "    company_df = pd.read_sql(\"SELECT company_name, MAX(version) as version FROM company GROUP BY company_name\", conn)\n",
    "\n",
    "# Step 3: Dynamically find CP files\n",
    "cp_files = {}\n",
    "for fname in os.listdir(cp_dir):\n",
    "    if fname.startswith(\"CP_Assessments_Regional\"):\n",
    "        cp_files[\"1\"] = os.path.join(cp_dir, fname)\n",
    "    elif fname.startswith(\"CP_Assessments\"):\n",
    "        cp_files[\"0\"] = os.path.join(cp_dir, fname)\n",
    "\n",
    "# Step 4: Process and insert data\n",
    "assessment_records = []\n",
    "alignment_records = []\n",
    "projection_records = []\n",
    "\n",
    "for is_regional, path in cp_files.items():\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df[\"company_name\"] = df[\"Company Name\"].str.strip()\n",
    "    df[\"assessment_date\"] = pd.to_datetime(df[\"Assessment Date\"], dayfirst=True, errors='coerce')\n",
    "    df[\"publication_date\"] = pd.to_datetime(df[\"Publication Date\"], errors='coerce')\n",
    "    df[\"projection_cutoff\"] = pd.to_datetime(df[\"History to Projection cutoff year\"], errors='coerce')\n",
    "    df[\"assumptions\"] = df.get(\"Assumptions\")\n",
    "    df[\"cp_unit\"] = df[\"CP Unit\"]\n",
    "    df[\"benchmark_id\"] = df.get(\"Benchmark ID\")\n",
    "    df[\"is_regional\"] = is_regional\n",
    "\n",
    "    assessment_df = df[[\n",
    "        \"company_name\", \"assessment_date\", \"publication_date\",\n",
    "        \"assumptions\", \"cp_unit\", \"projection_cutoff\", \"benchmark_id\", \"is_regional\"\n",
    "    ]].merge(company_df, on=\"company_name\", how=\"inner\")\n",
    "\n",
    "    assessment_records.append(assessment_df)\n",
    "\n",
    "    # CP Alignment columns\n",
    "    align_cols = [col for col in df.columns if col.startswith(\"Carbon Performance Alignment \")]\n",
    "    for col in align_cols:\n",
    "        year = int(col.split()[-1])\n",
    "        temp = df[[\"company_name\", \"assessment_date\"]].copy()\n",
    "        temp[\"cp_alignment_year\"] = year\n",
    "        temp[\"cp_alignment_value\"] = df[col]\n",
    "        temp[\"is_regional\"] = is_regional\n",
    "        temp = temp.merge(company_df, on=\"company_name\", how=\"inner\")\n",
    "        temp = temp.dropna(subset=[\"cp_alignment_value\"])\n",
    "        alignment_records.append(temp)\n",
    "\n",
    "    # CP Projection columns\n",
    "    year_cols = [col for col in df.columns if re.fullmatch(r\"\\d{4}\", col)]\n",
    "    for col in year_cols:\n",
    "        year = int(col)\n",
    "        temp = df[[\"company_name\", \"assessment_date\"]].copy()\n",
    "        temp[\"cp_projection_year\"] = year\n",
    "        temp[\"cp_projection_value\"] = df[col]\n",
    "        temp[\"is_regional\"] = is_regional\n",
    "        temp = temp.merge(company_df, on=\"company_name\", how=\"inner\")\n",
    "        temp = temp.dropna(subset=[\"cp_projection_value\"])\n",
    "        projection_records.append(temp)\n",
    "\n",
    "\n",
    "# Step 5: Insert into DB\n",
    "pd.concat(assessment_records).to_sql(\"cp_assessment\", engine, if_exists=\"append\", index=False)\n",
    "pd.concat(alignment_records).to_sql(\"cp_alignment\", engine, if_exists=\"append\", index=False)\n",
    "pd.concat(projection_records).to_sql(\"cp_projection\", engine, if_exists=\"append\", index=False)\n",
    "print(\"✅ CP tables created and populated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
